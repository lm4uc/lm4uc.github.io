Title,Authors,Paper_URL
BAID: A Benchmark for Bias Assessment of AI Detectors,Priyam Basu,https://openreview.net/forum?id=WMpNB7vvWX
Lost in Localization: Building RabakBench with Human-in-the-Loop Validation to Expose Multilingual Safety Gaps,"Gabriel Chua, Leanne Tan, Ziyu Ge, Roy Ka-Wei Lee",https://openreview.net/forum?id=62cQZD2lXa
SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth,"Wenpeng Xing, Lanyi Wei, Haixiao Hu, Rongchang Li, Mohan Li, Changting Lin, Meng Han",https://openreview.net/forum?id=xSw9HhyRw9
Pluralistic AI Alignment: A Cross-Cultural Pilot Survey,"Khashayar Alavi, Lucie Flek, Florian Mai",https://openreview.net/forum?id=A9oz6qFlQ4
Bias Dynamics in BabyLMs: Towards a Compute-Efficient Sandbox for Democratising Pre-Training Debiasing,"Filip Trhl√≠k, Andrew Caines, Paula Buttery",https://openreview.net/forum?id=ggtPjOqy2G
Language Models Entangle Language and Culture,"Shourya Jain, Paras Chopra",https://openreview.net/forum?id=2BQ1Rfy7VO
UnWEIRDing LLM Entity Recommendations,"Sanket Mhatre, Aayush Kumar",https://openreview.net/forum?id=Z6VoAn0diz
UbuntuGuard: A Policy-Based Safety Benchmark for Low-Resource African Languages,"Tassallah Abdullahi, Macton Mgonzo, Abraham Toluwase Owodunni, Ritambhara Singh, Carsten Eickhoff",https://openreview.net/forum?id=uPSzx3SBdf
Jo.E(Joint Evaluation) : A Multi-Agent Collaborative Framework for Comprehensive AI Safety Evaluation of Language Models,Himanshu Joshi,https://openreview.net/forum?id=f3oydpXzMM
